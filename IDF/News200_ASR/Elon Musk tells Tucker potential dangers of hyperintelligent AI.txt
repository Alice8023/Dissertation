 So all of a sudden, AI is everywhere. People who weren't quite sure what it was or playing with it on their phones. Is that good or bad? Yeah, so I've been thinking about AI for a long time since I was in college, really. It was one of the things that, just sort of four or five things, I thought, would really affect the future dramatically. It is fundamentally profound in that the smartest creatures, as far as you know, on this Earth are humans. Is our defining characteristic. We obviously weaker than, say, chimpanzees and less agile, but real smarter. So now, what happens when something vastly smarter than the smartest person comes along and is looking for. It's very difficult to predict what will happen in that circumstance. It's called the singularity. It's a singularity like a black hole. You don't know what happens after that. It's hard to predict. So I think we should be cautious with AI. And we should, I think there should be some government oversight because it affects the, it's a danger to the public. And so when you have things that are a danger to the public, let's say food and drugs, I thought we had the Food and Drug Administration, right, and the Federal Aviation Administration, the FCC. We have these agencies to oversee things that affect the public, where they could be public harm. And you don't want companies cutting corners on safety, and then having people suffer as a result. So that's why I've actually for a long time been a strong advocate of AI regulation. So that I think regulation is, you know, it's not fun to be regulated. It's sort of somewhat, somewhat odd to be regulated. I have a lot of experience with regulated industries because obviously automotive is highly regulated. You can fold this room with all the regulations that are required for a production car just in the United States. And then there's a whole different set of regulations in Europe and China and the rest of the world. So very familiar with being overseen by a lot of regulators. And the same thing is true with rockets. You can't just woolly nearly shoot rockets off, but not big ones anyway. Because the FAA is oversees that. And then even to get a launch license, you either have probably half a dozen or more federal agencies that need to approve it, plus state agencies. So I've been through so many regulatory situations. And sometimes I, people think I'm sort of like regulatory maverick that sort of defies regulators on a regular basis. But this is actually not the case. But in, you know, once in a blue moon rarely, I will disagree with regulators. But the vast majority of the time my company's agree with regulations and comply. As an anyway, so I think I think we should take this seriously. And we should have a regulatory agency. I think it needs to start with a group that initially seeks insight into AI. And then solicits opinion from industry. And then has proposed rulemaking. And then those rules, you know, will probably hopefully gradually be accepted by the major players in AI. And I think we'll have a better chance of advanced AI being beneficial to humanity in that circumstance. But all regulations start with a perceived danger. Plains fall out of the sky. Food causes botchless. Yes. I don't think the average person playing with AI on his iPhone perceives any danger. Can you just roughly explain what you think the dangers might be? Yes. So the danger really, AI is perhaps more dangerous than, say, mismanaged aircraft design or production maintenance. Or bad car production in the sense that it is. It has the potential. How the small one make regard that probability, but it is non-trivial. It has the potential of a civilizational destruction. This maybe is like terminator, but it wouldn't quite happen like terminator. Because the intelligence would be in the data centers. Right. The robust just the end effector. But I think perhaps what you may be alluded to here is that regulations are really only pointed to a factor. After something terrible has happened. That's correct. If that's the case for AI and we only put a regulations after something terrible has happened, it may be too late to actually put the regulations in place. The AI may be in control at that point. You think that's real. It is conceivable that AI could take control and reach a point where you couldn't turn it off and it would be making decisions for people. Yeah. Absolutely. Absolutely. No, that's definitely where things are headed. For sure. I mean, the things I could say, chat GBT, which is based on JPD4 from OpenAI, which is a company that I played a critical role in creating, unfortunately. Back when it was a non-profit? Yes. I mean, the reason OpenAI exists at all is that Larry Page and I used to be close friends and I was there at his house in Palo Alto. And I would talk to him late to the night about AI safety. And at least my perception was that Larry was not taking AI safety seriously enough. What did he say about it? He really seemed to be one of one sort of digital superintelligence, basically digital God, if you will, as soon as possible. He wanted that? Yes. He's made many public statements over the years that the whole goal of Google is what's called AI artificial general intelligence or artificial superintelligence. No, and I agree with him that there's great potential for good, but there's also potential for bad. And so if you've got some radical new technology, you want to try to take the set of actions that maximize probably it will do good and minimize probably it will do bad things. Yes. It can't just be helpful, either. It just goes, you know, barreling forward and, you know, hope for the best. And then at one point, I said, well, what about, you know, who we're going to make sure humanity's okay here? And, and, and, and, and then he called me a species. Did he use that term? Yes. And there were witnesses that I was the only one there when he called me a species. And so I was like, okay, that's it. I have, yes, I'm a species. Okay. You got me. What are you? Yeah, I'm fully a species. Busted. So that was the last straw. At the time, Google had a quite deep mind. And so Google and DeepMind together had about three quarters of all the AI talent in the world. They obviously had a transplant of money and more computers than anyone else. So I'm like, okay, we're about unipolar world here where there's just one one company that it has closed a monopoly on AI talent and, and computers like scale computing and person who's in in charge doesn't seem to care about safety. This is not good. So, so I thought, what's the, what's the, the furthest thing from Google would be like a nonprofit that is fully open because Google was closed for profit. So that's why the opening of an AI refers to open source, you know, transparency. So people know what's going on. Yes. And that we don't want to have like a, I mean, while I'm normally in favor of full profit, we don't want this to be sort of a profit maximizing a theme in from hell. That's right. That just never stops. Right. So that's how open AI was. So you want species incentives here. Incentives that like, I think we want to pro human. Yeah. So, I just like the future good for the humans. Yes. Yes. Because we're humans. So can you just put it, I keep pressing it, but just, just for people who haven't thought this through and aren't familiar with it. And the cool parts of, of artificial intelligence are so obvious. You know, write your college paper for you, write a limbic about yourself. Wait, there's a lot there that's fun and useful. But can you be more precise about what's potentially dangerous and scary? Like, what could it do? What specifically are you worried about? Going with old saying, the pen is mighty than the sword. So, if you have a super intelligent AI that is capable of writing incredibly well and in a way that is very influential, you know, convincing. And then, and is constantly figuring out what is more convincing to people over time and then enter a social learning. And then, enter a social media, for example, Twitter, but also Facebook and others, you know. And potentially manipulates public opinion in a way that is very bad. How would we even know? How do we even know? So, to sum up, in the words of Elon Musk, for all human history, human beings have been the smartest beings on the planet. Now, human beings have created something that is far smarter than they are. And the consequences of that are impossible to predict. And the people who created it don't care. In fact, as he put it, Google Founder Larry Page, a former friend of his, is looking to build a quote, digital god, and believes that anybody who's worried about that is a species. In other words, is looking out for human beings first. Elon Musk responded as a human being, it's okay to look out for human beings first. And then at the end, he said, the real problem with AI is not simply that it will jump the boundaries and become autonomous and you can't turn it off. In the short term, the problem with AI is that it might control your brain through words. And this is the application that we need to worry about now, particularly going into the next presidential election. The Democratic Party, as usual, was ahead of the curve on this. They've been thinking about how to harness AI for political power. More on that next.